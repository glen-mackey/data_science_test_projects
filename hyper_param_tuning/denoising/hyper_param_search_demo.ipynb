{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1f0c86-4abc-4e45-b8f4-4f073a7445eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyper-parameter Search Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228e2d1-079c-4637-a143-37a9c5c631c2",
   "metadata": {},
   "source": [
    "Note: This is a work in progress and documentation is not complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fb260-4242-4e9a-ada8-29286e4a461b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to project\n",
    "\n",
    "### Background\n",
    "\n",
    "Most machine learning applications require hyper-parameter tuning.  The typical approach is to input a range of values for each hyper-parameter, the search creates a grid of all of the combinations of the differnet hyper-parameters, then the model is solved for each of the hyper-parameter combinations in the grid and some metric is use to pick the \"best\" model.  Regardless, hyper-parameter tuning can require significant computing resources because the expectation is to try a large number models with the expectation of picking the best one.  If a model takes 1 minute to run, but there are 60 hyper-parameter combinations, then the comput time would be 1 hr.  For continuous hyper-parameters, e.g., regularization parameter, it is possible to build an adaptive search, but that is beyond the scope of this project.\n",
    "\n",
    "There are a number of different strategies for picking the \"best\" model.  In machine learning the most common approach is cross-validation (CV), where the data is split into x folds, and there are x repititions of using x-1 folds to train a model and using the left out fold to evaluate the fit.  The left out fold rotates in each repitition so that a prediction is made for each data point, but this approach requires fitting several models for each hyper-parameter combination in the grid, and increases computation time.\n",
    "\n",
    "In inverse modeling, the cross validation approach can't be employed, so other approaches are used.  Akaike information criterion (AIC), Bayesian information criterion (BIC), generalized cross validation (GCV), and un-biased predictive risk estimator (UPRE) are examples approaches the can be used for different types of problems.  GCV and UPRE can be very expensive to compute because they require explicitly computing a pseudo-inverse and influence matrix so approximation methods, e.g., http://brendt.wohlberg.net/publications/pdf/lin-2010-upre.pdf, may be required for large date sets.\n",
    "\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "The goal of this project is to demonstrate the importance of hyper-paramter tuning.  A second goal it to show how to actually implement a hyper-parameter tuning strategy (many people just count on `sklearn.model_selection.gridsearchcv()` to do most of the work).\n",
    "\n",
    "This project uses a classic denoising problem to demonstrate the importance of hyper-parameter tuning.  This type of problem uses a regularized inversion to estimate  the true data values from a noisy time series.  The solution of this type of problem is almost wholly dependent on the regularization parameter value used, and to lesser extent the order of the finite difference operator employed in the regularization.  Other problems show a strong dependence on the hyper-parameters used, but this problem goes from a useless fit to a nearly perfect fit back to a useless fit over the range of hyper-parameters used.\n",
    "\n",
    "This type of problem doesn't lend itself to using CV to select the best model.  A typical linear regression can use 4-folds to parameterize the model and then evaluate the fit of the predictions on the left out fold.  Basic denoising problems don't create a continous function in the same way as linear regression, so predicting data points that are not measured isn't really a possiblity without interpolating existing data points; this type of problem simply smoothes the data points in existing timeseries without parameterizing some prediction function.  \n",
    "\n",
    "\n",
    "This project creates a noisy data time series and attempt to de-noise it using regularized smoothing.  Because `gridsearchcv()` can't be used to do the hyper-parameter search for this type of problem, the grid search is explicitly implemented here.  The script converts the input hyper-paramter values into a grid, loops through that grid, and evaluates each model using the GCV metric; GCV is meant to approximate leave-one-out (LOO) cross-validation.  All of the models fits are stored and an interactive plot is provided so the user can explore just how dependent the fit is on the regularization parameter value used.\n",
    "\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "The project is structured into five sections: generating forward data, code to run the de-noising algorithm, creating the hyper-parameter grid and fitting the de-noising model to the data, code to create the visualizations, and the interactive implementation.  The sections need to be run in order.  The forward data is generated and model fitting is completely run and stored prior to visualization.  Because The model must be completely fit to create the scoring plots, the interactive graphs at the end simply reference the stored models and do not call functions to compute model fits in real time.  \n",
    "\n",
    "Scroll to the bottom of this page use this algorithm, or slowly work your way down to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54995fad-9b4c-4952-8202-c59974c164f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preamble\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from scipy import sparse as sp\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm._instances.clear()\n",
    "\n",
    "#sets the number of jobs to parallel process\n",
    "#-1 corresponds to the maximum number of cores available\n",
    "from joblib import Parallel, delayed\n",
    "n_jobs_use=-1\n",
    "\n",
    "from functools import cached_property\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42a23c-7e6a-4c52-8222-f0bebebd2e10",
   "metadata": {},
   "source": [
    "## Forward Model Data\n",
    "\n",
    "This section creates and plots some forward model data.  The data is based on a sine wave, with noised added to it, and includes the option to vary the amplitude of the wave over the data set.  `cycles`, `num_data_points`, `noise_level`, `y_coeff`,and `y_scaling` can be adjusted by the user to change the nature of the forward data.  Overall, a base sine wave with `num_data_points` is generated, this sine wave is skewed using `y_scaling`, the overall magnitude is adjusted with `y_coeff`, and normally distributed noise with standard deviation `noise_level` is added.  The plot at this section overlays the true data set on top of the noisy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c90a9-a1ef-4ddd-95ba-18a35c48d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward data\n",
    "\n",
    "#BEGIN USER INPUT SECTION\n",
    "\n",
    "#number of cycles in the sine wave\n",
    "cycles=2\n",
    "\n",
    "#number of data points to evaluate the sine wave function\n",
    "num_data_points=2_000\n",
    "\n",
    "#specifies the standard deviation of the normaly distrbuted noise to add to data\n",
    "noise_level=1\n",
    "\n",
    "#multiplier to apply to the base data, effectively plays against noise_level\n",
    "y_coeff=1\n",
    "\n",
    "#linear multiplier for the data set so that the magnitude of the sine wave is not constant\n",
    "#across the data set\n",
    "y_scaling=np.linspace(1,5,num_data_points)\n",
    "\n",
    "\n",
    "#creates the x input vector to go cycles number of sine wave cycles\n",
    "#with num_data_points number of data points\n",
    "x=np.linspace(0,2*math.pi*cycles,num_data_points)\n",
    "\n",
    "#END USER INPUT SECTION\n",
    "\n",
    "#evaluates the sine function on the x vector and adjusts the magnitude with the scaling vector\n",
    "y_base= y_scaling * np.sin(x)\n",
    "\n",
    "#creates random normal noise with a fixed seed \n",
    "#and adds noise to the y-data set after adjusting the noise-less data with y_coeff\n",
    "rng = default_rng(0)\n",
    "y_noise=rng.normal(loc=0,scale=noise_level,size=y_base.shape)\n",
    "y=y_coeff * y_base + y_noise\n",
    "\n",
    "\n",
    "#creates a plot to show the noiseless and noisy data\n",
    "#I have found that lines work better than dots for this type of scatter plot\n",
    "#because the noisy data effectively creates a solid background upon which to view the noiseless data\n",
    "#the color scheme here is the same as the color scheme later\n",
    "fig={}\n",
    "fig['base_data_plot']=make_subplots(rows=1, cols=1)\n",
    "#fig_base_data.update_layout(height=4 * 96 * 1, width=9.5 * 96 * 1)\n",
    "fig['base_data_plot'].add_scatter(x=x,y=y,name='y',mode='lines')\n",
    "fig['base_data_plot'].add_scatter(x=x, y=y_base, name = 'y true', mode='lines', line=dict(width=3, color='white'))\n",
    "#fig['base_data_plot'].update_layout(plot_bgcolor='white')\n",
    "\n",
    "#the default plot color (fig['base_data_plot']['layout']['plot_bgcolor']) is white, so I am not sure why it shows as a blue-grey\n",
    "fig['base_data_plot'].update_layout(plot_bgcolor='lightgrey')\n",
    "#print(fig['base_data_plot']['layout']['plot_bgcolor'])\n",
    "fig['base_data_plot'].update_layout(legend=dict(bgcolor=fig['base_data_plot']['layout']['plot_bgcolor']))\n",
    "#fig['base_data_plot'].update_layout(legend=dict(bgcolor=None))\n",
    "#fig['base_data_plot'].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6fc8d-43ea-4200-bf55-83a5c71a9f49",
   "metadata": {},
   "source": [
    "## Minimization Function and Solver\n",
    "\n",
    "This section contains the code to solve the de-noising problem and to compute the \"score\" for each of the candidate models in the hyper-parameter tuning set.  \n",
    "\n",
    "### Minimization Problem\n",
    "\n",
    "The minimization function to solve is\n",
    "$$\n",
    "\\min_\\hat{d} \\dfrac{1}{2} \\left\\|\\hat{d}-d\\right\\|^2_2 + \\dfrac{\\lambda}{2} \\left\\|D_k \\hat{d}\\right\\|^2_2,\n",
    "$$\n",
    "where $d$ is the noisy data, $\\hat{d}$ is the reconstruction of the true data, and $D_k$ is a finite difference operator of order $k$, e.g.,\n",
    "$$\n",
    "D_1=\n",
    "\\begin{bmatrix}\n",
    "-1 &  1 & 0 &       & \\\\\n",
    " 0 & -1 & 1 & 0     & \\\\\n",
    "   &    &   &       & \\ddots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "D_2=\n",
    "\\begin{bmatrix}\n",
    "1 & -2 &  1 & 0 &       & \\\\\n",
    "0 &  1 & -2 & 1 & 0     & \\\\\n",
    "  &    &    &   &       & \\ddots\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Dimensions on $D_k$ are $\\left[\\left(n-k\\right) x\\ n\\right]$.\n",
    "\n",
    "\n",
    "To simplify solving this problem we can re-write it as \n",
    "$$\n",
    "\\min_\\hat{d} \\dfrac{1}{2}\n",
    "\\left\\|\n",
    "\\begin{bmatrix}\n",
    "I_n\\\\\n",
    "\\sqrt{\\lambda}D_k\n",
    "\\end{bmatrix} \\hat{d}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "d\\\\\n",
    "0_{n-k}\n",
    "\\end{bmatrix}\n",
    "\\right\\|^2_2\n",
    "$$\n",
    "where $I_n$ is the identity matrix corresponding to the length of $d$ and $0_{n-k}$ is a column vector of zeros with length $n-k$, the same number of rows as in $D_k$.\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "Because this model can not predict new data in the same manner as a non-linear regression, the CV method for model selection can not be used here.  Instead we use the GCV method, and pick the model that has the minimum value for $S_\\lambda$, where\n",
    "$$\n",
    "S_\\lambda\n",
    "=\n",
    "\\dfrac{\n",
    "\\dfrac{1}{n}\n",
    "\\left\\|R_\\lambda\\right\\|^2_2\n",
    "}\n",
    "{\n",
    "\\left[\n",
    "\\dfrac{1}{n}\n",
    "\\text{trace}\n",
    "\\left(\n",
    "I_n-T_\\lambda\n",
    "\\right)\n",
    "\\right]^2\n",
    "}\n",
    "$$\n",
    "where are the model residuals at a given $\\lambda$ value, $R_\\lambda=\\hat{d}-d$, and $T_\\lambda$ is the corresponding influence matrix\n",
    "$$\n",
    "T_\\lambda\n",
    "=\n",
    "A \\left(A^T A + \\lambda R^T R\\right)^{-1} A^T,\n",
    "$$\n",
    "which for this problems simplifies to \n",
    "$$\n",
    "T_\\lambda\n",
    "=\n",
    "I_n \\left(I_n + \\lambda {D_k}^T D_k\\right)^{-1} {I_n}^T.\n",
    "$$\n",
    "\n",
    "### Section Structure\n",
    "\n",
    "This section contains code to compute a trailing finite difference operator based on an arbitary stencil.  This section begins by computing the stencil based on the order of the finite difference operator, computing the finite difference coefficients, and creates an appropriately dimensioned diagonal band matrix with these coefficients.  A series of hierarchical classes are created to solve the general regularized least squares problem and then layer in L2 total variation regularization and the denoising problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c1f2b-bcd5-4f0a-b54a-7ecbe0ce25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff_coeff(s,d):\n",
    "#https://en.wikipedia.org/wiki/Finite_difference_coefficient\n",
    "#finite difference coefficient vector\n",
    "#s is the stencil points, e.g., [-1,0,1], as a row vector\n",
    "#d is the derivative order\n",
    "#retuns a column vector\n",
    "    \n",
    "    n=len(s)\n",
    "    LHS=np.power(s,np.arange(n).reshape(n,1))\n",
    "    RHS=math.factorial(d)*np.concatenate([np.zeros(d),np.array(1,ndmin=1),np.zeros(n-d-1)])\n",
    "    coeff=np.linalg.solve(LHS,RHS).reshape(1,n)\n",
    "    \n",
    "    return coeff\n",
    "    \n",
    "def trailing_small_finite_diff_matrix(k,order=1,n=1):\n",
    "    #https://en.wikipedia.org/wiki/Finite_difference_coefficient\n",
    "    #k = the length of the dataset\n",
    "    #order is the order fo the finite difference derivative\n",
    "    #n is the number of stencil points, and relates to the derivative order and desired accuracy\n",
    "    \n",
    "    #print(k)\n",
    "    \n",
    "    #ensures num stencil points is n>=d+1\n",
    "    #for this application typically use n=d+1\n",
    "    n=max(n,order+1)\n",
    "    #print(n)\n",
    "    \n",
    "    #computes trailing stencil, end is exclusive so use 1 instead of 0\n",
    "    #this is the simplest solution that scales well\n",
    "    #since we only want the sum of the derivatives, and we don't care about the derivative at a single point, \n",
    "    #the actual indexing of the derivative is irrelevant\n",
    "    s=np.arange(-n+1,1)\n",
    "    #print(s)\n",
    "    \n",
    "    #compute trailing finite difference coefficeints\n",
    "    D_coeff=finite_diff_coeff(s,order)\n",
    "    #print(D_coeff)\n",
    "    \n",
    "    #puts everything into band matrix\n",
    "    #this is almost always a sparse matrix and benefits from sparse implementation\n",
    "    D=sp.spdiags((np.dot(np.ones((k,1)),D_coeff)).T,range(n), k-(n-1), k)\n",
    "    #D=sp.spdiags((np.dot(np.ones((k,1)),D_coeff)).T,range(n), k-(n-1), k).todense()\n",
    "    \n",
    "    return D\n",
    "\n",
    "class L2_resid_L2_reg():\n",
    "    \n",
    "    #this class solves L2 residual - L2 regularization problem for linear inversion/regression\n",
    "    #takes a linear operator, regularization operator, and regularization parameter and fits\n",
    "    #the model and returns the parameter values\n",
    "    #currently it also returns the Generalize Cross Validation (GCV score), but that can be expanded\n",
    "    \n",
    "    def __init__(self,A,R,lam=0, store_score_matrix=False):\n",
    "        \n",
    "        #initializes the model with the\n",
    "        #A=linear operator\n",
    "        #R=regularization operator (I am not sure that it supports no regularization at this point)\n",
    "        #lam=regularization parameter\n",
    "        #store_score_matrix = boolean to say whether or not to store matrices associated with the GCV score\n",
    "            #these matrices can take up a lot of memory (large full matrices) so sometimes long term storage isn't feasible\n",
    "        \n",
    "        self.A=A\n",
    "        self.R=R\n",
    "        self.lam=lam\n",
    "        self.store_score_matrix=store_score_matrix\n",
    "        #debug()\n",
    "        \n",
    "        #return self doesn't play well with the super class\n",
    "       # return self\n",
    "    \n",
    "    def fit(self,y):\n",
    "        #fits the regularized linear model to data set y\n",
    "        \n",
    "        #A = linear blurring operator\n",
    "        #y = true data\n",
    "        #R = linear regularization operator\n",
    "        #lam=regularization parameter value\n",
    "        \n",
    "        self.y=y\n",
    "\n",
    "        #debug()\n",
    "        #if one or both of the matrices is full, the operator_stack is full\n",
    "        #otherwise operator_stack is sparse, assumption is if one of the operators\n",
    "        #is full then it is for a reason\n",
    "        #could add logic to check the expected sparsity of keep sparse\n",
    "        #sp.vstack((full,sparse))=sparse\n",
    "        #this would be much easier in MATLAB\n",
    "        if isinstance(self.A,np.ndarray):\n",
    "            #full A, convert R to full\n",
    "            self.operator_stack=np.vstack((self.A,np.sqrt(self.lam)*self.R.A))\n",
    "            \n",
    "        elif isinstance(self.A,np.ndarray):\n",
    "            #full R, convert A to full\n",
    "            self.operator_stack=np.vstack((self.A.A,np.sqrt(self.lam)*self.R))\n",
    "            \n",
    "        elif isinstance(self.A,np.ndarray) and isinstance(self.F,np.ndarray):\n",
    "            #both full, no conversion\n",
    "            self.operator_stack=np.vstack((self.A,np.sqrt(self.lam)*self.R))\n",
    "        else:\n",
    "            #both sparse, no conversion\n",
    "            self.operator_stack=sp.vstack((self.A,np.sqrt(self.lam)*self.R))\n",
    "        \n",
    "        #assumption is b is full\n",
    "        self.b=np.concatenate((self.y,np.zeros((self.R.shape[0]))))\n",
    "        #print(lam)\n",
    "        \n",
    "        #stores the shape of the matrix as [p x q]\n",
    "        self.p,self.q=self.A.shape\n",
    "    \n",
    "        #print(operator_stack.shape)\n",
    "        #print(y.shape)\n",
    "        #print(np.zeros((R.shape[0])).shape)\n",
    "        #print(np.concatenate((y,np.zeros((R.shape[0])))).shape)\n",
    "    \n",
    "        \n",
    "        #solves the least squares problem using lstsq or lsqr\n",
    "        #I think these functions use a QR or related decomposition and\n",
    "        #benefit from not explicitly forming a Hessian matrix which squares the condition number\n",
    "        \n",
    "        #ignoring the 1/2, which would go in lsqr as np.sqrt(1/2)\n",
    "        #doing this changes the objective function value, but not the location of the minimum\n",
    "        if isinstance(self.operator_stack, np.ndarray):\n",
    "            #full operator_stack\n",
    "            self.full_model=np.linalg.lstsq(self.operator_stack,self. b)\n",
    "        else:  \n",
    "            #sparse operator_stack\n",
    "            self.full_model=sp.linalg.lsqr(self.operator_stack,self.b)#,x0=y)\n",
    "        #full_model=sp.linalg.lsqr(np.sqrt(1/2)*operator_stack,np.sqrt(1/2)*b);\n",
    "        #full_model=np.linalg.lstsq(operator_stack.A,b)  #slow\n",
    "        \n",
    "        #stores the model fit and condition number in explicit attributes\n",
    "        self.m=self.full_model[0]\n",
    "        self.acond=self.full_model[6]\n",
    "        \n",
    "        #evaluates the predicted data and the regularization vector\n",
    "        self.yhat = self.A @ self.m\n",
    "        self.Reg_m = self.R @ self.m\n",
    "        \n",
    "        #computes residuals and the norm of the residuals\n",
    "        self.resid = self.yhat-y\n",
    "        self.norm_resid = np.linalg.norm(self.resid)\n",
    "        \n",
    "        return self\n",
    "    #print(y.shape)\n",
    "    #print(model['yhat'].shape)\n",
    "    \n",
    "    def pseudo_inv_fun(self):\n",
    "        \n",
    "        #function to compute the pseudo_inv for the regularized linear problem\n",
    "        #only use this in evaluating fit score because of potential for \n",
    "        #numerical instability\n",
    "        \n",
    "        #put this in it's own function so I can call it for the cached property function\n",
    "        #as well as the memory shortcut application without having to duplicate all of the if-thens\n",
    "        \n",
    "        #using sparse solver because the LHS is sparse\n",
    "        #even though both inputs are sparse the output of this will be full\n",
    "        #converting the RHS to full here is actually faster than if I kept it as sparse\n",
    "        #i.e., sp.linalg.spsolve(operator_stack.T @ operator_stack, A.T)\n",
    "        #and overall this is faster than doing all of the calculations as sparse and converting to full\n",
    "        #i.e., sp.linalg.spsolve(operator_stack.T @ operator_stack, A.T).todense()\n",
    "        #this is what I had observed in matlab\n",
    "        #model['pseudo_inv']= sp.linalg.spsolve(operator_stack.T @ operator_stack, A.A.T)\n",
    "        \n",
    "        \n",
    "        if isinstance(self.operator_stack,np.ndarray):\n",
    "            #operator_stack is full, A is sparse\n",
    "            #A needs to be full for np.linalg.solve()\n",
    "            return np.linalg.solve(self.operator_stack.T @ self.operator_stack, self.A.A.T)\n",
    "        \n",
    "        elif isinstance(self.A,np.ndarray):\n",
    "            #operator_stack is sparse, A is already full\n",
    "            #which if good for sp.linalg.spsolve, but A.A doesn't exist if A is already full\n",
    "            return sp.linalg.spsolve(self.operator_stack.T @ self.operator_stack, self.A.T)\n",
    "                \n",
    "        elif isinstance(self.operator_stack,np.ndarray) and isinstance(self.A,np.ndarray):\n",
    "            #both operator_stack and A are full\n",
    "            return np.linalg.solve(self.operator_stack.T @ self.operator_stack, self.A.T)\n",
    "        else:   \n",
    "            #both operator_stack and A are sparse\n",
    "            #if A is sparse then this solves faster if convert A to full first\n",
    "            #also sp.linalg.spsolve(sparse,full)=full, which is better for later computations\n",
    "            return sp.linalg.spsolve(self.operator_stack.T @ self.operator_stack, self.A.A.T)\n",
    "    \n",
    "    #@cached_property computes and stores class attributes on demand\n",
    "    #the first time the attribute is called it is computed and then stored as an attribute\n",
    "    #https://stackoverflow.com/questions/1598174/pythonic-way-to-only-do-work-first-time-a-variable-is-called\n",
    "    #the function name becomes the attribute name\n",
    "    \n",
    "    @cached_property\n",
    "    def pseudo_inv(self):\n",
    "        #computes and stores the pseudo inverse matrix (AtA + lam*RtR)*At\n",
    "        #evaluates and stores the pseudo inverse from pseudo_inv_fun in self.pseudo_inv\n",
    "        #this can take up a lot of memory so sometimes it should be avoided\n",
    "        \n",
    "        #return sp.linalg.spsolve(self.operator_stack.T @ self.operator_stack, self.A.A.T)\n",
    "        return self.pseudo_inv_fun()\n",
    "    \n",
    "\n",
    "    @cached_property\n",
    "    def influence_matrix(self):\n",
    "        #computes and stores the influence matrix A*(AtA + lam*RtR)*At\n",
    "        #this can take up a lot of memory so sometimes it should be avoided\n",
    "        \n",
    "        #here sparse @ full = full and is faster than full @ full\n",
    "        #I am not sure if this is what I saw in Matlab, and it may not scale well\n",
    "        #for a less sparse A, but it works here\n",
    "        #model['influence_matrix'] = A.A @ model['pseudo_inv']\n",
    "        #model['influence_matrix'] = A @ model['pseudo_inv']\n",
    "        \n",
    "        #if self.store_score_matrix:\n",
    "        return self.A @ self.pseudo_inv\n",
    "    \n",
    "    @cached_property\n",
    "    def GCV_val(self):\n",
    "        #debug()\n",
    "        #computes and stores the Generalized Cross Validation (GCV) score value\n",
    "        \n",
    "        if self.store_score_matrix:\n",
    "            #uses stored values for influence_matrix and by proxy for the pseudo inverse\n",
    "            #doint this can take up a lot of memory\n",
    "            local_influence_matrix=self.influence_matrix\n",
    "        else:\n",
    "            #calculates pseudo inverse and influence_matrix but does not store them\n",
    "            #lose calculated information, but requires less memory for large datasets\n",
    "            \n",
    "            local_influence_matrix=self.A @ self.pseudo_inv_fun()\n",
    "            \n",
    "            # #pseudo_inv=sp.linalg.spsolve(self.operator_stack.T @ self.operator_stack, self.A.A.T)\n",
    "            # #influence_matrix=self.A @ pseudo_inv\n",
    "            # if isinstance(self.operator_stack,np.ndarray):\n",
    "            #     #operator_stack is full, A is sparse\n",
    "            #     #if A is sparse it is faster to convert A to full\n",
    "            #     local_influence_matrix=self.A @ np.linalg.spsolve(self.operator_stack.T @ self.operator_stack, self.A.A.T):\n",
    "            \n",
    "            # elif isinstance(self.A,np.ndarray):\n",
    "            #     #operator_stack is sparse, A is already full\n",
    "            #     local_influence_matrix=self.A @ sp.linalg.spsolve(self.operator_stack.T @ self.operator_stack, self.A.T):\n",
    "                    \n",
    "            # elif isinstance(self.operator_stack,np.ndarray) and isinstance(self.A,np.ndarray):\n",
    "            #     #both operator_stack and A are full\n",
    "            #     local_influence_matrix=self.A @ np.linalg.spsolve(self.operator_stack.T @ self.operator_stack, self.A.T):\n",
    "            # else:   \n",
    "            #     #both operator_stack and A are sparse\n",
    "            #     #if A is sparse it is faster to convert A to full\n",
    "            #     local_influence_matrix=self.A @ sp.linalg.spsolve(self.operator_stack.T @ self.operator_stack, self.A.A.T)\n",
    "            \n",
    "        #debug()\n",
    "        return 1/self.p * self.norm_resid ** 2 \\\n",
    "                /(1-np.trace(local_influence_matrix)/self.p) ** 2\n",
    "                #/(1/model['p'] * np.trace(np.eye(len(y))-model['influence_matrix'])) ** 2 is\n",
    "                #slower because you have to form I, do the full subtraction, and then take the trace\n",
    "                #trace(I_p) is just p\n",
    "                \n",
    "    \n",
    "    def GCV_val_calc(self):\n",
    "        #this is a function call version of GCV_val for use with joblib.parallel\n",
    "        return self.GCV_val\n",
    "    #debug()\n",
    "    \n",
    "class L2_L2_TV_debulurring(L2_resid_L2_reg):\n",
    "    #class the for the case of L2_resid_L2_reg where you are using L2 D_n @ m style regularization\n",
    "    def __init__(self,A,order=1,lam=0, store_score_matrix=False):\n",
    "        \n",
    "        #initializes a model a L2_resid_L2_reg with finite difference regularization\n",
    "        #finite difference order is set in order\n",
    "        #this type of problem can be used for L2-TV based de-noising and deblurring\n",
    "        \n",
    "        #gets shape of linear operator for residual portion of problem\n",
    "        #uses this shape to create appropriately sized regularization operator\n",
    "        p,q=A.shape\n",
    "        \n",
    "        #create appropriately sized regularization operator\n",
    "        #this is sparse diagonal band matrix\n",
    "        R=trailing_small_finite_diff_matrix(k=p,order=order)\n",
    "        \n",
    "        \n",
    "        #initiates this class by calling the parent class-L2_resid_L2_reg\n",
    "        #sends the linear operator, regularization operator, and lambda value to the parent class\n",
    "        super().__init__(A,R,lam,store_score_matrix)\n",
    "        \n",
    "       # return self\n",
    "        \n",
    "\n",
    "class L2_L2_TV_denoising(L2_L2_TV_debulurring):\n",
    "    #class the for the case of L2_resid_L2_reg where you are using\n",
    "    #L2 I @ m resiguals and\n",
    "    #L2 D_n @ m style regularization\n",
    "    \n",
    "    def __init__(self,p,order=1,lam=0, store_score_matrix=False):\n",
    "        \n",
    "        #initializes model by layering de-blurring operator on top of \n",
    "        #L2-TV regularization\n",
    "        #in this class we specify a de-blurring operator that will be sent to\n",
    "        #the de-blurring/de-noising class whose parent is the base L2-L2 solver class\n",
    "        \n",
    "        #creates sparse identity matrix to match size of input data\n",
    "        #this is the linear operator\n",
    "        A=sp.eye(p)\n",
    "\n",
    "        #initiates this class by calling the parent class-L2_L2_TV_debulurring\n",
    "        #sends the linear operator, finite difference operator order, and lambda value to the parent class\n",
    "        super().__init__(A,order,lam,store_score_matrix)\n",
    "        \n",
    "        #return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940cff1-d60d-4258-bb45-658684269c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates and runs the hyper parameter search\n",
    "\n",
    "min_log_lam=-15\n",
    "max_log_lam=15\n",
    "step_log_lam=0.5\n",
    "#str(step_log_lam)[::-1].find('.') reverses the order and then counts characters to the decimal, finds the number of decimal places\n",
    "log_lam=np.arange(min_log_lam,max_log_lam+10*np.spacing(max_log_lam),step_log_lam).round(str(step_log_lam)[::-1].find('.'))\n",
    "\n",
    "min_finite_diff_order=1\n",
    "max_finite_diff_order=2\n",
    "#finite_diff_operator={'D%d'%(i): trailing_small_finite_diff_matrix(k=len(y),order=i) \n",
    " #                     for i in range(min_finite_diff_order,max_finite_diff_order+1)}\n",
    "finite_diff_operator_order={'D%d'%(i): i for i in range(min_finite_diff_order,max_finite_diff_order+1)}\n",
    "\n",
    "#combines lambda and the finite difference operator into a parameter grid\n",
    "#this variable is just for display\n",
    "#parameter_grid_dict_display={'log_lam':list(log_lam),\n",
    " #                            'R': finite_diff_operator.keys()}\n",
    "parameter_grid_dict_display={'log_lam':list(log_lam),\n",
    "                             'R': finite_diff_operator_order.keys()}\n",
    "    \n",
    "\n",
    "#this variable is for actual use\n",
    "#parameter_grid_dict={'lam':list(10**log_lam),\n",
    " #                    'R': finite_diff_operator.values()}\n",
    "parameter_grid_dict={'lam':list(10**log_lam),\n",
    "                     'order': finite_diff_operator_order.values()}\n",
    "\n",
    "\n",
    "def parameter_grid_to_list_of_param_dict(param_grid):\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    return [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "parameter_grid_list_display=parameter_grid_to_list_of_param_dict(parameter_grid_dict_display)\n",
    "parameter_grid_list=parameter_grid_to_list_of_param_dict(parameter_grid_dict)\n",
    "\n",
    "#prints the dictionary of list\n",
    "pprint.pprint(parameter_grid_dict_display,sort_dicts=False, compact=True)\n",
    "print()\n",
    "#prints the list of dictionaries\n",
    "pprint.pprint(parameter_grid_list_display,sort_dicts=False, compact=True)\n",
    "\n",
    "#model_list=[solve_denoising(sp.eye(len(y)),y,**reg_params) for reg_params in tqdm(parameter_grid_list)]\n",
    "model_list=[L2_L2_TV_denoising(len(y),**reg_params,store_score_matrix=False).fit(y) for reg_params in tqdm(parameter_grid_list)]\n",
    "#I have been tinkering with implementing parallel delayed for this type of user defined class \n",
    "#but I haven't had much luck so far.  I have gotten a lot of #PicklingError: Could not pickle the task to send it to the workers.\n",
    "#On Binder I am not sure if parallel processing would make much difference, but I would like to figure this out\n",
    "#https://stackoverflow.com/questions/50528331/parallel-class-function-calls-using-python-joblib\n",
    "#https://stackoverflow.com/questions/50528331/parallel-class-function-calls-using-python-joblib\n",
    "#model_list_base=[L2_L2_TV_denoising(len(y),**reg_params,store_score_matrix=False) for reg_params in tqdm(parameter_grid_list)]\n",
    "#model_list=Parallel(n_jobs=n_jobs_use)(delayed(model.fit)(y) for model in tqdm(model_list_base))\n",
    "\n",
    "\n",
    "#work around to deal with problem of not being able to load class values,\n",
    "#model_list_dict=[model.__dict__ for model in model_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703aff7-b412-466a-bc85-815720f3e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivots results into two level dictionary \n",
    "#with 1) difference operator type, 2) log reg prameter\n",
    "#this is not a general solution \n",
    "\n",
    "model_dict_plotting={finite_diff_operator_name:\n",
    "                     {setup_dict['log_lam']:model for setup_dict,model in zip(parameter_grid_list_display,model_list) \n",
    "                      if setup_dict['R']==finite_diff_operator_name}\n",
    "                     for finite_diff_operator_name in finite_diff_operator_order.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d7b06-f1d9-4ceb-97ce-4f26e16b0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for plotting\n",
    "\n",
    "#plt.plot([model['GCV_val'] for model in model_dict_plotting['D2'].values()])\n",
    "\n",
    "#score_plot_arr=np.array([[model['GCV_val'] for model in model_dict_plotting[diff_operator_name].values()] \n",
    " #                        for diff_operator_name in model_dict_plotting.keys()]).T\n",
    "score_plot_arr=np.array([[model.GCV_val for model in tqdm(model_dict_plotting[diff_operator_name].values())] \n",
    "                         for diff_operator_name in model_dict_plotting.keys()]).T\n",
    "\n",
    "score_plot_arr.shape\n",
    "\n",
    "#START HERE\n",
    "\n",
    "#print({key: score_plot_arr[:,i] for i, key in enumerate(finite_diff_operator)})\n",
    "\n",
    "base10_plot_df=pd.DataFrame({'x': log_lam, **{key: score_plot_arr[:,i] for i, key in enumerate(finite_diff_operator_order)}})\n",
    "log10_plot_df=pd.DataFrame({'x': log_lam, **{key: np.log10(score_plot_arr[:,i]) for i, key in enumerate(finite_diff_operator_order)}})\n",
    "\n",
    "def plot_x_y(diff_operator_name, log_lam_val):\n",
    "    \n",
    "    plot_x_y_df=pd.DataFrame({'x':x,\n",
    "                              'y':y,\n",
    "                              'y hat': model_dict_plotting[diff_operator_name][round(log_lam_val,str(step_log_lam)[::-1].find('.'))].yhat,\n",
    "                              'y true': y_base})\n",
    "                              #'y hat': model_dict_plotting[diff_operator_name][log_lam_val]['yhat']})\n",
    "    \n",
    "    fig['denoised_data_plot']=make_subplots(rows=1, cols=1)\n",
    "    #fig.update_layout(height=8 * 96 * 1, width=9.5 * 96 * 1, )\n",
    "    \n",
    "    #fig = px.add_scatter(plot_x_y_df, x='x', y='y',\n",
    "     #               labels={'x':'x', 'value':'y', \"variable\": \"data type\"}).update_traces(mode='lines')\n",
    "    fig['denoised_data_plot'].add_scatter(x=plot_x_y_df['x'], y=plot_x_y_df['y'],name = 'y',mode='lines')#,line=dict(color='blue'))\n",
    "    \n",
    "    fig['denoised_data_plot'].add_scatter(x=plot_x_y_df['x'], y=plot_x_y_df['y hat'], name = 'y hat', mode='lines', line=dict(width=4))#, color='red'))\n",
    "    \n",
    "    fig['denoised_data_plot'].update_layout(plot_bgcolor='lightgrey')\n",
    "    #print(fig['base_data_plot']['layout']['plot_bgcolor'])\n",
    "    fig['denoised_data_plot'].update_layout(legend=dict(bgcolor=fig['denoised_data_plot']['layout']['plot_bgcolor']))\n",
    "    \n",
    "    #marker_spacing=3\n",
    "    #fig.add_scatter(x=plot_x_y_df['x'][marker_spacing-1::marker_spacing], y=plot_x_y_df['y true'][marker_spacing-1::marker_spacing], \n",
    "     #               name = 'y true', mode='markers', marker=dict(color='white',\n",
    "      #                                                           size=5,\n",
    "       #                                                          line=dict(color='black',\n",
    "        #                                                                   width=0.5)))\n",
    "                    \n",
    "    fig['denoised_data_plot'].add_scatter(x=plot_x_y_df['x'], y=plot_x_y_df['y true'], name = 'y true', mode='lines',line=dict(width=2, color='white',dash = 'dash'))\n",
    "    \n",
    "\n",
    "    fig['denoised_data_plot'].show()\n",
    "    return\n",
    "\n",
    "#init_guess_ind=np.argmin(np.array([model['GCV_val'] for model in model_list]))\n",
    "init_guess_ind=np.argmin(np.array([model.GCV_val for model in model_list]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5259229-6afd-41e8-a474-b029b11994c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#does the plotting and the interaction\n",
    "\n",
    "#plot_df\n",
    "plot_df=base10_plot_df\n",
    "#plot_df=log10_plot_df\n",
    "fig['score_plot']=px.scatter(plot_df,\n",
    "                             x='x', \n",
    "                             y=plot_df.columns[1:],\n",
    "                             labels={'x':'log10 lambda', 'value':'GCV score', \"variable\": \"Diff Op\"}).update_traces(mode='lines')\n",
    "#fig1.update_layout(height=4 * 96 * 1, width=9.5 * 96 * 1)\n",
    "fig['score_plot'].update_layout(plot_bgcolor='lightgrey')\n",
    "print(fig['base_data_plot']['layout']['plot_bgcolor'])\n",
    "fig['score_plot'].update_layout(legend=dict(bgcolor=fig['score_plot']['layout']['plot_bgcolor']))\n",
    "fig['score_plot'].show()\n",
    "#model_dict_plotting.keys()\n",
    "\n",
    "log_lam_widget=widgets.FloatSlider(min=min_log_lam, max=max_log_lam, step=step_log_lam, \n",
    "                                   value=parameter_grid_list_display[init_guess_ind]['log_lam'],\n",
    "                                   description='log10 lambda:',)\n",
    "diff_operator_widget=widgets.Dropdown(options=model_dict_plotting.keys(),\n",
    "                                      value=parameter_grid_list_display[init_guess_ind]['R'],\n",
    "                                      description='Finite diff operator:')\n",
    "\n",
    "interact(plot_x_y,\n",
    "        diff_operator_name=diff_operator_widget,\n",
    "        log_lam_val=log_lam_widget)\n",
    "\n",
    "print('The last plot can be very touchy in Binder; it works consistently locally in JupyterLab.')\n",
    "print('If the plot is not updating when you change the input values re-run the last cell.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
